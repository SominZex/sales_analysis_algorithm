{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13aec7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939275eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129e740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connector imported successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from connector import get_db_connection\n",
    "    print(\"Database connector imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing database connector: {e}\")\n",
    "    print(\"Make sure your connector.py is in the path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773426c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch unique products\n",
    "def fetch_all_products():\n",
    "    \"\"\"Fetch all unique products from your database.\"\"\"\n",
    "    try:\n",
    "        engine = get_db_connection()\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT DISTINCT productName \n",
    "        FROM sales_data \n",
    "        WHERE productName IS NOT NULL \n",
    "        AND productName != ''\n",
    "        ORDER BY productName\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql(query, engine)\n",
    "        engine.dispose()\n",
    "        \n",
    "        products = df['productName'].tolist()\n",
    "        print(f\"Fetched {len(products)} unique products from database\")\n",
    "        return products\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching products: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9885ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sample_sales_data():\n",
    "    \"\"\"Fetch sample sales data for testing consolidation.\"\"\"\n",
    "    try:\n",
    "        engine = get_db_connection()\n",
    "        \n",
    "        # Get latest date\n",
    "        last_date_query = \"SELECT MAX(orderDate) FROM product_sales;\"\n",
    "        last_date = pd.read_sql(last_date_query, engine).iloc[0, 0]\n",
    "        \n",
    "        # Get top 50 products by sales for the latest date\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            productName AS `Product Name`,\n",
    "            totalProductPrice,\n",
    "            quantity AS `Quantity Sold`\n",
    "        FROM sales_data \n",
    "        WHERE orderDate = %s\n",
    "        AND productName IS NOT NULL\n",
    "        ORDER BY totalProductPrice DESC\n",
    "        LIMIT 50\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql(query, engine, params=(last_date,))\n",
    "        engine.dispose()\n",
    "        \n",
    "        # Add serial number\n",
    "        df.insert(0, 'S.No', range(1, len(df) + 1))\n",
    "        \n",
    "        print(f\"Fetched sample sales data: {len(df)} products\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching sales data: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89747128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading product data from database...\n",
      "Fetched 47490 unique products from database\n",
      "Fetched sample sales data: 50 products\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading product data from database...\")\n",
    "products = fetch_all_products()\n",
    "sample_sales = fetch_sample_sales_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1fc753",
   "metadata": {},
   "source": [
    "#### Excel Export Refined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcc875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Clustering similar product names...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24524/24524 [18:36<00:00, 21.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to top_products_july2025.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "MYSQL_USER = \"root\"\n",
    "MYSQL_PASS = \"root\"\n",
    "MYSQL_HOST = \"localhost\"\n",
    "MYSQL_PORT = 3306\n",
    "MYSQL_DB   = \"sales_data\"\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASS}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}\"\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT productName, totalProductPrice, quantity, brandName, categoryName\n",
    "FROM sales_data\n",
    "WHERE orderDate BETWEEN '2025-07-01' AND '2025-07-31'\n",
    "AND productName IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, engine)\n",
    "df['totalProductPrice'] = pd.to_numeric(df['totalProductPrice'], errors='coerce')\n",
    "df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')\n",
    "df.dropna(subset=['totalProductPrice', 'quantity'], inplace=True)\n",
    "\n",
    "def normalize_name(name):\n",
    "    return (\n",
    "        str(name).lower()\n",
    "        .replace(\"‚Ñ¢\", \"\")\n",
    "        .replace(\"¬Æ\", \"\")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"&\", \"and\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "df['cleanName'] = df['productName'].apply(normalize_name)\n",
    "unique_clean_names = df['cleanName'].unique().tolist()\n",
    "\n",
    "name_map = df[['cleanName', 'productName']].drop_duplicates().set_index('cleanName')['productName'].to_dict()\n",
    "\n",
    "clusters = []\n",
    "visited = set()\n",
    "\n",
    "print(\"Clustering similar product names...\")\n",
    "\n",
    "for name in tqdm(unique_clean_names):\n",
    "    if name in visited:\n",
    "        continue\n",
    "    cluster = [name]\n",
    "    visited.add(name)\n",
    "    for other in unique_clean_names:\n",
    "        if other in visited:\n",
    "            continue\n",
    "        score = fuzz.token_sort_ratio(name, other)\n",
    "        if score >= 90:\n",
    "            cluster.append(other)\n",
    "            visited.add(other)\n",
    "    clusters.append(cluster)\n",
    "\n",
    "grouped_data = []\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_df = df[df['cleanName'].isin(cluster)]\n",
    "    total_sales = cluster_df['totalProductPrice'].sum()\n",
    "    total_qty = cluster_df['quantity'].sum()\n",
    "\n",
    "    representative = cluster_df['productName'].mode().iloc[0]\n",
    "    top_brand = cluster_df['brandName'].mode().iloc[0] if not cluster_df['brandName'].isna().all() else ''\n",
    "    top_category = cluster_df['categoryName'].mode().iloc[0] if not cluster_df['categoryName'].isna().all() else ''\n",
    "\n",
    "    grouped_data.append({\n",
    "        'Product Name': representative,\n",
    "        'Brand': top_brand,\n",
    "        'Category': top_category,\n",
    "        'Total Sales': round(total_sales, 2),\n",
    "        'Quantity Sold': int(total_qty)\n",
    "    })\n",
    "\n",
    "result_df = pd.DataFrame(grouped_data)\n",
    "result_df.sort_values(by='Total Sales', ascending=False, inplace=True)\n",
    "result_df.insert(0, 'S.No', range(1, len(result_df) + 1))\n",
    "\n",
    "MAX_ROWS = 1000000\n",
    "num_sheets = math.ceil(len(result_df) / MAX_ROWS)\n",
    "\n",
    "with pd.ExcelWriter(\"top_products_july2025.xlsx\", engine='openpyxl') as writer:\n",
    "    for i in range(num_sheets):\n",
    "        sheet_df = result_df.iloc[i * MAX_ROWS : (i\n",
    "                                                   + 1) * MAX_ROWS]\n",
    "        sheet_df.to_excel(writer, sheet_name=f\"Top Products {i+1}\", index=False)\n",
    "\n",
    "print(\"Exported to top_products_july2025.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86957666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ab011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10a8da1de9349829ff3750992298974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/36653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\it\\Documents\\sales_analysis_algorithm\\vmac\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASS}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}\"\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT productName, totalProductPrice, quantity, brandName, categoryName\n",
    "FROM sales_data\n",
    "WHERE orderDate BETWEEN '2025-02-01' and '2025-07-31'\n",
    "AND productName IS NOT NULL\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "df['totalProductPrice'] = pd.to_numeric(df['totalProductPrice'], errors='coerce')\n",
    "df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')\n",
    "df.dropna(subset=['totalProductPrice', 'quantity'], inplace=True)\n",
    "df['text'] = df.apply(lambda row: f\"{row['productName']} | {row['brandName']} | {row['categoryName']}\", axis=1)\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "faiss.normalize_L2(embeddings)\n",
    "index.add(embeddings)\n",
    "\n",
    "threshold = 0.9\n",
    "visited = set()\n",
    "clusters = []\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    _, neighbors = index.search(np.expand_dims(embeddings[i], axis=0), len(embeddings))\n",
    "    cluster = []\n",
    "    for j in neighbors[0]:\n",
    "        if j not in visited and np.dot(embeddings[i], embeddings[j]) >= threshold:\n",
    "            cluster.append(j)\n",
    "            visited.add(j)\n",
    "    clusters.append(cluster)\n",
    "\n",
    "results = []\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_df = df.iloc[cluster]\n",
    "    total_sales = cluster_df['totalProductPrice'].sum()\n",
    "    total_qty = cluster_df['quantity'].sum()\n",
    "    top_name = cluster_df['productName'].mode().iloc[0]\n",
    "    top_brand = cluster_df['brandName'].mode().iloc[0] if not cluster_df['brandName'].isna().all() else ''\n",
    "    top_category = cluster_df['categoryName'].mode().iloc[0] if not cluster_df['categoryName'].isna().all() else ''\n",
    "\n",
    "    results.append({\n",
    "        \"Product Name\": top_name,\n",
    "        \"Brand\": top_brand,\n",
    "        \"Category\": top_category,\n",
    "        \"Total Sales\": round(total_sales, 2),\n",
    "        \"Quantity Sold\": int(total_qty)\n",
    "    })\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.sort_values(by='Total Sales', ascending=False, inplace=True)\n",
    "result_df.insert(0, 'S.No', range(1, len(result_df) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f497e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to top_products_july2025_llm.xlsx\n"
     ]
    }
   ],
   "source": [
    "result_df.to_excel(\"top_products_july2025_llm.xlsx\", index=False)\n",
    "print(\"Exported to top_products_july2025_llm.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f1282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Summary:\n",
      "   Total unique products: 47490\n",
      "   Sample products:\n",
      "      1. \tMCVITTIES DIGSTV BAR CHOC 30GM\n",
      "      2.           Slow Kiss Perfume for Men - Aromatic Fresh - 100ml\n",
      "      3.      Zandu dant veer\n",
      "      4.     lighter clipper\n",
      "      5.    Marlboro Clove \n",
      "      6.    TEDDY MARSH MELLO 16G \n",
      "      7.   GHEE BATTI\n",
      "      8.   SAATHI SPIRAL SOFT MRP180\n",
      "      9.   Tofu Soya Paneer\n",
      "      10.  1 pcs Classic Mild \n",
      "      ... and 47480 more\n"
     ]
    }
   ],
   "source": [
    "if products:\n",
    "    print(f\"Database Summary:\")\n",
    "    print(f\"   Total unique products: {len(products)}\")\n",
    "    print(f\"   Sample products:\")\n",
    "    for i, product in enumerate(products[:10], 1):\n",
    "        print(f\"      {i}. {product}\")\n",
    "        \n",
    "    if len(products) > 10:\n",
    "        print(f\"... and {len(products)-10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e3358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductSimilarityModel:\n",
    "    \"\"\"\n",
    "    A model to find semantic similarity between products and group similar products together.\n",
    "    This helps in analyzing product families rather than individual SKUs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold: float = 0.7):\n",
    "        \"\"\"Initialize the model with similarity threshold.\"\"\"\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.vectorizer = None\n",
    "        self.product_vectors = None\n",
    "        self.product_names = None\n",
    "        self.similarity_matrix = None\n",
    "        self.product_groups = None\n",
    "        self.group_representatives = None\n",
    "\n",
    "    def preprocess_product_name(self, name: str) -> str:\n",
    "        \"\"\"Clean and preprocess product names for better similarity matching.\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        name = name.lower()\n",
    "        \n",
    "        # Remove specific measurement patterns found in your data\n",
    "        patterns_to_remove = [\n",
    "            # Handle decimal measurements like 80.00gm, 1000.00ml, 21.00gm\n",
    "            r'\\b\\d+\\.\\d+\\s*(gm|ml|kg|mg|oz|lbs?|pcs?|pieces?|g)\\b',\n",
    "            \n",
    "            # Handle integer measurements like 250ml, 20pcs, 10s\n",
    "            r'\\b\\d+\\s*(gm|ml|kg|mg|oz|lbs?|pcs?|pieces?|g)\\b',\n",
    "            \n",
    "            # Handle measurements without decimal like 250ml, 20pcs\n",
    "            r'\\b\\d+\\s*(ml|mg|g|gm|kg|oz|lbs?|pcs?|pieces?|pack|box|can|bottle|packet)\\b',\n",
    "            \n",
    "            # Handle combinations like \"250ml x 6\", \"10 x 20gm\"\n",
    "            r'\\b\\d+(\\.\\d+)?\\s*x\\s*\\d+(\\.\\d+)?\\s*(ml|mg|g|gm|kg|oz|lbs?|pcs?)\\b',\n",
    "            \n",
    "            # Handle size indicators\n",
    "            r'\\b(small|medium|large|xl|xs|s|m|l)\\b',\n",
    "            \n",
    "            # Handle standalone numbers like \"10s\", \"20\", \"250\"\n",
    "            r'\\b\\d+s?\\b',\n",
    "            \n",
    "            # Handle specific patterns like \"10S\", \"20pcs\"\n",
    "            r'\\b\\d+[sS]\\b',\n",
    "            \n",
    "            # Handle weight/volume without units that might be left\n",
    "            r'\\b\\d+(\\.\\d+)?\\b',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns_to_remove:\n",
    "            name = re.sub(pattern, '', name)\n",
    "        \n",
    "        # Normalize common terms\n",
    "        name = re.sub(r'\\bcigarette\\b', 'cigarette', name)\n",
    "        name = re.sub(r'\\bcigarettes\\b', 'cigarette', name)\n",
    "        name = re.sub(r'\\benergy drink\\b', 'energy drink', name)\n",
    "        \n",
    "        # Remove extra spaces and special characters except spaces\n",
    "        name = re.sub(r'[^\\w\\s]', ' ', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        \n",
    "        return name\n",
    "\n",
    "    def extract_features(self, products: List[str]) -> np.ndarray:\n",
    "        \"\"\"Extract TF-IDF features from product names.\"\"\"\n",
    "        # Preprocess product names\n",
    "        processed_products = [self.preprocess_product_name(name) for name in products]\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            min_df=1,\n",
    "            max_df=0.8,\n",
    "            lowercase=True,\n",
    "            token_pattern=r'\\b[a-zA-Z][a-zA-Z]+\\b'\n",
    "        )\n",
    "        \n",
    "        # Fit and transform the data\n",
    "        self.product_vectors = self.vectorizer.fit_transform(processed_products)\n",
    "        \n",
    "        return self.product_vectors\n",
    "    \n",
    "    def calculate_similarity_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Calculate cosine similarity matrix between all products.\"\"\"\n",
    "        if self.product_vectors is None:\n",
    "            raise ValueError(\"Please extract features first using extract_features()\")\n",
    "            \n",
    "        self.similarity_matrix = cosine_similarity(self.product_vectors)\n",
    "        return self.similarity_matrix\n",
    "    \n",
    "    def find_similar_products(self, product_index: int, top_k: int = 10) -> List[Tuple[int, str, float]]:\n",
    "        \"\"\"Find top-k similar products for a given product.\"\"\"\n",
    "        if self.similarity_matrix is None:\n",
    "            self.calculate_similarity_matrix()\n",
    "            \n",
    "        similarities = self.similarity_matrix[product_index]\n",
    "        \n",
    "        similar_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in similar_indices:\n",
    "            if similarities[idx] >= self.similarity_threshold:\n",
    "                results.append((idx, self.product_names[idx], similarities[idx]))\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def group_similar_products(self) -> Dict[int, List[int]]:\n",
    "        \"\"\"Group similar products using clustering.\"\"\"\n",
    "        if self.similarity_matrix is None:\n",
    "            self.calculate_similarity_matrix()\n",
    "        \n",
    "        similarity_matrix = np.clip(self.similarity_matrix, 0, 1)\n",
    "        distance_matrix = 1 - similarity_matrix\n",
    "\n",
    "        eps = 1 - self.similarity_threshold\n",
    "        clustering = DBSCAN(eps=eps, min_samples=1, metric='precomputed')\n",
    "        cluster_labels = clustering.fit_predict(distance_matrix)\n",
    "\n",
    "        self.product_groups = {}\n",
    "        for idx, label in enumerate(cluster_labels):\n",
    "            if label not in self.product_groups:\n",
    "                self.product_groups[label] = []\n",
    "            self.product_groups[label].append(idx)\n",
    "\n",
    "        return self.product_groups\n",
    "    \n",
    "    def get_group_representatives(self) -> Dict[int, Tuple[int, str]]:\n",
    "        \"\"\"Get representative product for each group.\"\"\"\n",
    "        if self.product_groups is None:\n",
    "            self.group_similar_products()\n",
    "            \n",
    "        self.group_representatives = {}\n",
    "        \n",
    "        for group_id, product_indices in self.product_groups.items():\n",
    "            if len(product_indices) == 1:\n",
    "                # Single product group\n",
    "                idx = product_indices[0]\n",
    "                self.group_representatives[group_id] = (idx, self.product_names[idx])\n",
    "            else:\n",
    "                # Find product with highest average similarity to other products in group\n",
    "                best_idx = None\n",
    "                best_avg_similarity = -1\n",
    "                \n",
    "                for idx in product_indices:\n",
    "                    avg_similarity = np.mean([self.similarity_matrix[idx][other_idx] \n",
    "                                            for other_idx in product_indices if other_idx != idx])\n",
    "                    if avg_similarity > best_avg_similarity:\n",
    "                        best_avg_similarity = avg_similarity\n",
    "                        best_idx = idx\n",
    "                \n",
    "                self.group_representatives[group_id] = (best_idx, self.product_names[best_idx])\n",
    "        \n",
    "        return self.group_representatives\n",
    "    \n",
    "    def fit(self, products: List[str]) -> 'ProductSimilarityModel':\n",
    "        \"\"\"Fit the model on product data.\"\"\"\n",
    "        self.product_names = products\n",
    "        \n",
    "        print(f\"Processing {len(products)} products...\")\n",
    "        \n",
    "        # Extract features\n",
    "        self.extract_features(products)\n",
    "        print(\"Features extracted\")\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        self.calculate_similarity_matrix()\n",
    "        print(\"Similarity matrix calculated\")\n",
    "        \n",
    "        # Group similar products\n",
    "        self.group_similar_products()\n",
    "        print(f\"Products grouped into {len(self.product_groups)} groups\")\n",
    "        \n",
    "        # Get group representatives\n",
    "        self.get_group_representatives()\n",
    "        print(\"Group representatives identified\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_consolidated_products(self, sales_data: pd.DataFrame, \n",
    "                                product_col: str = 'Product Name',\n",
    "                                sales_col: str = 'Sales',\n",
    "                                quantity_col: str = 'Quantity Sold') -> pd.DataFrame:\n",
    "        \"\"\"Consolidate similar products and aggregate their sales/quantities.\"\"\"\n",
    "        if self.product_groups is None or self.group_representatives is None:\n",
    "            raise ValueError(\"Please fit the model first\")\n",
    "        \n",
    "        product_to_representative = {}\n",
    "        \n",
    "        for group_id, product_indices in self.product_groups.items():\n",
    "            representative_idx, representative_name = self.group_representatives[group_id]\n",
    "            \n",
    "            for product_idx in product_indices:\n",
    "                original_product = self.product_names[product_idx]\n",
    "                product_to_representative[original_product] = representative_name\n",
    "        \n",
    "        consolidated_data = sales_data.copy()\n",
    "        consolidated_data['Consolidated_Product'] = consolidated_data[product_col].map(\n",
    "            product_to_representative\n",
    "        ).fillna(consolidated_data[product_col])\n",
    "        \n",
    "        aggregated_data = consolidated_data.groupby('Consolidated_Product').agg({\n",
    "            sales_col: 'sum',\n",
    "            quantity_col: 'sum'\n",
    "        }).reset_index()\n",
    "\n",
    "        aggregated_data.columns = ['Product Name', 'Sales', 'Quantity Sold']\n",
    "        aggregated_data = aggregated_data.sort_values('Sales', ascending=False).reset_index(drop=True)\n",
    "        aggregated_data.insert(0, 'S.No', range(1, len(aggregated_data) + 1))\n",
    "\n",
    "        return aggregated_data\n",
    "\n",
    "    def analyze_groups(self) -> pd.DataFrame:\n",
    "        \"\"\"Analyze the groups formed and return summary statistics.\"\"\"\n",
    "        if self.product_groups is None:\n",
    "            raise ValueError(\"Please fit the model first\")\n",
    "\n",
    "        analysis_data = []\n",
    "\n",
    "        for group_id, product_indices in self.product_groups.items():\n",
    "            representative_idx, representative_name = self.group_representatives[group_id]\n",
    "\n",
    "            group_products = [self.product_names[idx] for idx in product_indices]\n",
    "\n",
    "            analysis_data.append({\n",
    "                'Group_ID': group_id,\n",
    "                'Representative_Product': representative_name,\n",
    "                'Group_Size': len(product_indices),\n",
    "                'Products_in_Group': ' | '.join(group_products)\n",
    "            })\n",
    "\n",
    "        analysis_df = pd.DataFrame(analysis_data)\n",
    "        analysis_df = analysis_df.sort_values('Group_Size', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        return analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84427787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocessing():\n",
    "    \"\"\"Test preprocessing with sample products.\"\"\"\n",
    "    print(\"TESTING PREPROCESSING WITH YOUR PRODUCT PATTERNS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Use actual samples from your data if available\n",
    "    if products:\n",
    "        test_products = products[:20]\n",
    "    else:\n",
    "        # Fallback sample products based on your patterns\n",
    "        test_products = [\n",
    "            \"Red Bull Energy Drink 250ml Can\",\n",
    "            \"Red Bull Energy Drink 80.00ml\",\n",
    "            \"Marlboro Advance Compact 10S\", \n",
    "            \"Marlboro Advance Compact 21.00gm\",\n",
    "            \"Wills Classic Connect Cigarette 20pcs\",\n",
    "            \"Wills Classic Connect Cigarette 50.00gm\",\n",
    "            \"Gold Flake Premium 10Pcs\",\n",
    "            \"Gold Flake Premium 1.00pcs\"\n",
    "        ]\n",
    "    \n",
    "    model = ProductSimilarityModel()\n",
    "    \n",
    "    preprocessing_results = []\n",
    "    print(\"Original Product Name ‚Üí Preprocessed Name\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for product in test_products:\n",
    "        preprocessed = model.preprocess_product_name(product)\n",
    "        preprocessing_results.append({\n",
    "            'Original': product,\n",
    "            'Preprocessed': preprocessed\n",
    "        })\n",
    "        print(f\"{product:<35} ‚Üí {preprocessed}\")\n",
    "    \n",
    "    return pd.DataFrame(preprocessing_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a022a695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING PREPROCESSING WITH YOUR PRODUCT PATTERNS\n",
      "======================================================================\n",
      "\n",
      "Original Product Name ‚Üí Preprocessed Name\n",
      "----------------------------------------------------------------------\n",
      "\tMCVITTIES DIGSTV BAR CHOC 30GM     ‚Üí mcvitties digstv bar choc\n",
      "          Slow Kiss Perfume for Men - Aromatic Fresh - 100ml ‚Üí slow kiss perfume for men aromatic fresh\n",
      "     Zandu dant veer                ‚Üí zandu dant veer\n",
      "    lighter clipper                 ‚Üí lighter clipper\n",
      "   Marlboro Clove                   ‚Üí marlboro clove\n",
      "   TEDDY MARSH MELLO 16G            ‚Üí teddy marsh mello\n",
      "  GHEE BATTI                        ‚Üí ghee batti\n",
      "  SAATHI SPIRAL SOFT MRP180         ‚Üí saathi spiral soft mrp180\n",
      "  Tofu Soya Paneer                  ‚Üí tofu soya paneer\n",
      " 1 pcs Classic Mild                 ‚Üí classic mild\n",
      " 45 San Vito Pasta Plain            ‚Üí san vito pasta plain\n",
      " 4700 Microwave Popcorn Natural Original Healthy Popcorn 85G ‚Üí microwave popcorn natural original healthy popcorn\n",
      " 8 -IN -1 MIX SEED&NUTS  250 GM     ‚Üí in mix seed nuts\n",
      " ACNO FIGHT FACE WASH               ‚Üí acno fight face wash\n",
      " All out Coil mosquito              ‚Üí all out coil mosquito\n",
      " Amul Chocolate Brownie Sandwich Gold Ice Cream 60Ml ‚Üí amul chocolate brownie sandwich gold ice cream\n",
      " Amul delicious spread              ‚Üí amul delicious spread\n",
      " Amul Ginger Tea Mix                ‚Üí amul ginger tea mix\n",
      " Amul Tricone Double Chocolate 120ml ‚Üí amul tricone double chocolate\n",
      " ANANDA VANILA MILK                 ‚Üí ananda vanila milk\n"
     ]
    }
   ],
   "source": [
    "# Run preprocessing test\n",
    "preprocessing_df = test_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbfa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_compare_thresholds(products_to_use, thresholds=[0.5, 0.6, 0.7, 0.8]):\n",
    "    \"\"\"Train models with different similarity thresholds and compare results.\"\"\"\n",
    "    print(f\"COMPARING SIMILARITY THRESHOLDS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    results = []\n",
    "    models = {}\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        print(f\"Training model with threshold {threshold}...\")\n",
    "        \n",
    "        model = ProductSimilarityModel(similarity_threshold=threshold)\n",
    "        model.fit(products_to_use)\n",
    "\n",
    "        analysis_df = model.analyze_groups()\n",
    "        multi_groups = len(analysis_df[analysis_df['Group_Size'] > 1])\n",
    "        single_groups = len(analysis_df[analysis_df['Group_Size'] == 1])\n",
    "        reduction_pct = ((len(products_to_use) - len(analysis_df)) / len(products_to_use)) * 100\n",
    "\n",
    "        results.append({\n",
    "            'Threshold': threshold,\n",
    "            'Total_Groups': len(analysis_df),\n",
    "            'Multi_Product_Groups': multi_groups,\n",
    "            'Single_Product_Groups': single_groups,\n",
    "            'Reduction_Percentage': reduction_pct\n",
    "        })\n",
    "\n",
    "        models[threshold] = model\n",
    "        print(f\"Created {len(analysis_df)} groups ({multi_groups} multi-product)\")\n",
    "\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    print(f\"THRESHOLD COMPARISON RESULTS:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    return comparison_df, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63febb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéöÔ∏è COMPARING SIMILARITY THRESHOLDS\n",
      "======================================================================\n",
      "\n",
      "üîÑ Training model with threshold 0.5...\n",
      "üîÑ Processing 100 products...\n",
      "‚úÖ Features extracted\n",
      "‚úÖ Similarity matrix calculated\n",
      "‚úÖ Products grouped into 98 groups\n",
      "‚úÖ Group representatives identified\n",
      "   ‚úÖ Created 98 groups (2 multi-product)\n",
      "\n",
      "üîÑ Training model with threshold 0.6...\n",
      "üîÑ Processing 100 products...\n",
      "‚úÖ Features extracted\n",
      "‚úÖ Similarity matrix calculated\n",
      "‚úÖ Products grouped into 99 groups\n",
      "‚úÖ Group representatives identified\n",
      "   ‚úÖ Created 99 groups (1 multi-product)\n",
      "\n",
      "üîÑ Training model with threshold 0.7...\n",
      "üîÑ Processing 100 products...\n",
      "‚úÖ Features extracted\n",
      "‚úÖ Similarity matrix calculated\n",
      "‚úÖ Products grouped into 99 groups\n",
      "‚úÖ Group representatives identified\n",
      "   ‚úÖ Created 99 groups (1 multi-product)\n",
      "\n",
      "üîÑ Training model with threshold 0.8...\n",
      "üîÑ Processing 100 products...\n",
      "‚úÖ Features extracted\n",
      "‚úÖ Similarity matrix calculated\n",
      "‚úÖ Products grouped into 99 groups\n",
      "‚úÖ Group representatives identified\n",
      "   ‚úÖ Created 99 groups (1 multi-product)\n",
      "\n",
      "üìä THRESHOLD COMPARISON RESULTS:\n",
      " Threshold  Total_Groups  Multi_Product_Groups  Single_Product_Groups  Reduction_Percentage\n",
      "       0.5            98                     2                     96                   2.0\n",
      "       0.6            99                     1                     98                   1.0\n",
      "       0.7            99                     1                     98                   1.0\n",
      "       0.8            99                     1                     98                   1.0\n"
     ]
    }
   ],
   "source": [
    "products_subset = products[:100] if len(products) > 100 else products\n",
    "threshold_comparison, trained_models = train_and_compare_thresholds(products_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10dd940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING MODEL WITH THRESHOLD 0.6\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "best_threshold = 0.6\n",
    "\n",
    "print(f\"USING MODEL WITH THRESHOLD {best_threshold}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trained_models[best_threshold]\n",
    "\n",
    "analysis_df = best_model.analyze_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c20feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUP ANALYSIS SUMMARY:\n",
      "   Total products analyzed: 100\n",
      "   Total groups created: 99\n",
      "   Multi-product groups: 1\n",
      "   Products consolidated: 1\n",
      "   Reduction percentage: 1.0%\n",
      "\n",
      "üîó TOP MULTI-PRODUCT GROUPS:\n"
     ]
    }
   ],
   "source": [
    "print(f\"GROUP ANALYSIS SUMMARY:\")\n",
    "print(f\"Total products analyzed: {len(products_subset)}\")\n",
    "print(f\"Total groups created: {len(analysis_df)}\")\n",
    "print(f\"Multi-product groups: {len(analysis_df[analysis_df['Group_Size'] > 1])}\")\n",
    "print(f\"Products consolidated: {len(products_subset) - len(analysis_df)}\")\n",
    "print(f\"Reduction percentage: {((len(products_subset) - len(analysis_df)) / len(products_subset) * 100):.1f}%\")\n",
    "\n",
    "print(f\"TOP MULTI-PRODUCT GROUPS:\")\n",
    "multi_groups = analysis_df[analysis_df['Group_Size'] > 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group -1 (2 products)\n",
      "   Representative:  ewen500g\n",
      "      1.  ewen500g\n",
      "      2.  GRENDER1600\n"
     ]
    }
   ],
   "source": [
    "for idx, row in multi_groups.iterrows():\n",
    "    print(f\"Group {row['Group_ID']} ({row['Group_Size']} products)\")\n",
    "    print(f\"   Representative: {row['Representative_Product']}\")\n",
    "    products_in_group = row['Products_in_Group'].split(' | ')\n",
    "    for i, product in enumerate(products_in_group, 1):\n",
    "        print(f\"{i}. {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de26fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING CONSOLIDATION WITH REAL SALES DATA\n",
      "======================================================================\n",
      "\n",
      "üìà Original Sales Data (Top 15):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Sales'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìà Original Sales Data (Top 15):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m display_cols = [\u001b[33m'\u001b[39m\u001b[33mS.No\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mProduct Name\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSales\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mQuantity Sold\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43msample_sales\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdisplay_cols\u001b[49m\u001b[43m]\u001b[49m.head(\u001b[32m15\u001b[39m).to_string(index=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Test consolidation\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\it\\Documents\\sales_analysis_algorithm\\vmac\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\it\\Documents\\sales_analysis_algorithm\\vmac\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\it\\Documents\\sales_analysis_algorithm\\vmac\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Sales'] not in index\""
     ]
    }
   ],
   "source": [
    "if not sample_sales.empty:\n",
    "    print(f\"TESTING CONSOLIDATION WITH REAL SALES DATA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"Original Sales Data (Top 15):\")\n",
    "    display_cols = ['S.No', 'Product Name', 'Sales', 'Quantity Sold']\n",
    "    print(sample_sales[display_cols].head(15).to_string(index=False))\n",
    "\n",
    "    try:\n",
    "        consolidated_sales = best_model.get_consolidated_products(\n",
    "            sample_sales,\n",
    "            product_col='Product Name',\n",
    "            sales_col='Sales', \n",
    "            quantity_col='Quantity Sold'\n",
    "        )\n",
    "\n",
    "        print(f\"Consolidated Sales Data (Top 15):\")\n",
    "        print(consolidated_sales[display_cols].head(15).to_string(index=False))\n",
    "        print(f\"CONSOLIDATION IMPACT:\")\n",
    "        print(f\"Original products: {len(sample_sales)}\")\n",
    "        print(f\"Consolidated products: {len(consolidated_sales)}\")\n",
    "        print(f\"Reduction: {((len(sample_sales) - len(consolidated_sales)) / len(sample_sales) * 100):.1f}%\")\n",
    "\n",
    "        original_total = sample_sales['Sales'].sum()\n",
    "        consolidated_total = consolidated_sales['Sales'].sum()\n",
    "        print(f\"Original total sales: ‚Çπ{original_total:,.2f}\")\n",
    "        print(f\"Consolidated total sales: ‚Çπ{consolidated_total:,.2f}\")\n",
    "        print(f\"Sales preservation: {(consolidated_total/original_total*100):.1f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Consolidation test failed: {e}\")\n",
    "else:\n",
    "    print(\"No sample sales data available for consolidation testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba435c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_sales_data(df):\n",
    "    \"\"\"\n",
    "    Standardize column names to work with the consolidation logic.\n",
    "    Ensures presence of: 'S.No', 'Product Name', 'Sales', 'Quantity Sold'\n",
    "    \"\"\"\n",
    "    rename_map = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_lower = col.strip().lower()\n",
    "        if 'product' in col_lower:\n",
    "            rename_map[col] = 'Product Name'\n",
    "        elif 'price' in col_lower or 'sales' in col_lower:\n",
    "            rename_map[col] = 'Sales'\n",
    "        elif 'qty' in col_lower or 'quantity' in col_lower:\n",
    "            rename_map[col] = 'Quantity Sold'\n",
    "    \n",
    "    df = df.rename(columns=rename_map)\n",
    "    \n",
    "    if 'S.No' not in df.columns:\n",
    "        df.insert(0, 'S.No', range(1, len(df) + 1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def test_product_consolidation(sample_sales, best_model):\n",
    "    \"\"\"\n",
    "    Run and print the product consolidation test using the provided model and sales data.\n",
    "    \"\"\"\n",
    "    if sample_sales.empty:\n",
    "        print(\"No sample sales data available for consolidation testing\")\n",
    "        return\n",
    "\n",
    "    sample_sales = standardize_sales_data(sample_sales)\n",
    "\n",
    "    print(f\"TESTING CONSOLIDATION WITH REAL SALES DATA\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    try:\n",
    "        display_cols = ['S.No', 'Product Name', 'Sales', 'Quantity Sold']\n",
    "        print(f\"Original Sales Data (Top 15):\")\n",
    "        print(sample_sales[display_cols].head(15).to_string(index=False))\n",
    "\n",
    "        consolidated_sales = best_model.get_consolidated_products(\n",
    "            sample_sales,\n",
    "            product_col='Product Name',\n",
    "            sales_col='Sales', \n",
    "            quantity_col='Quantity Sold'\n",
    "        )\n",
    "\n",
    "        print(f\"Consolidated Sales Data (Top 15):\")\n",
    "        print(consolidated_sales[display_cols].head(15).to_string(index=False))\n",
    "\n",
    "        print(f\"CONSOLIDATION IMPACT:\")\n",
    "        print(f\"Original products: {len(sample_sales)}\")\n",
    "        print(f\"Consolidated products: {len(consolidated_sales)}\")\n",
    "        reduction_pct = ((len(sample_sales) - len(consolidated_sales)) / len(sample_sales)) * 100\n",
    "        print(f\"Reduction: {reduction_pct:.1f}%\")\n",
    "\n",
    "        original_total = sample_sales['Sales'].sum()\n",
    "        consolidated_total = consolidated_sales['Sales'].sum()\n",
    "        print(f\"Original total sales: ‚Çπ{original_total:,.2f}\")\n",
    "        print(f\"Consolidated total sales: ‚Çπ{consolidated_total:,.2f}\")\n",
    "        print(f\"Sales preservation: {(consolidated_total / original_total * 100):.1f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Consolidation test failed: {e}\")\n",
    "        print(\"Columns in sample_sales:\", sample_sales.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fdade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(threshold_comparison, analysis_df, sample_sales, consolidated_sales):\n",
    "    \"\"\"Create visualizations for the model results.\"\"\"\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Groups vs Threshold', 'Reduction % vs Threshold', \n",
    "                       'Group Size Distribution', 'Sales Comparison'),\n",
    "        specs=[[{\"secondary_y\": True}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=threshold_comparison['Threshold'], \n",
    "                  y=threshold_comparison['Total_Groups'],\n",
    "                  name='Total Groups', line=dict(color='blue')),\n",
    "                row=1, col=1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=threshold_comparison['Threshold'], \n",
    "                  y=threshold_comparison['Multi_Product_Groups'],\n",
    "                  name='Multi-Product Groups', line=dict(color='red')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=threshold_comparison['Threshold'], \n",
    "               y=threshold_comparison['Reduction_Percentage'],\n",
    "               name='Reduction %', marker_color='green'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    group_sizes = analysis_df['Group_Size'].value_counts().sort_index()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=group_sizes.index, y=group_sizes.values,\n",
    "               name='Group Frequency', marker_color='orange'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    if not sample_sales.empty and 'consolidated_sales' in locals():\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=['Original', 'Consolidated'], \n",
    "                   y=[len(sample_sales), len(consolidated_sales)],\n",
    "                   name='Product Count', marker_color='purple'),\n",
    "            row=2, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(height=800, showlegend=True, \n",
    "                     title_text=\"Product Similarity Model Analysis Results\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe2a5b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà TESTING CONSOLIDATION WITH REAL SALES DATA\n",
      "======================================================================\n",
      "\n",
      "üìà Original Sales Data (Top 15):\n",
      "‚ùå Consolidation test failed: \"['Sales'] not in index\"\n",
      "üßæ Columns in sample_sales: ['S.No', 'Product Name', 'Product Name', 'Quantity Sold']\n"
     ]
    }
   ],
   "source": [
    "test_product_consolidation(sample_sales, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Visualization skipped - consolidated sales data not available\n"
     ]
    }
   ],
   "source": [
    "# visualize\n",
    "try:\n",
    "    if 'consolidated_sales' in locals():\n",
    "        create_visualizations(threshold_comparison, analysis_df, sample_sales, consolidated_sales)\n",
    "    else:\n",
    "        print(\"Visualization skipped - consolidated sales data not available\")\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb99db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model function\n",
    "def save_model(model, threshold):\n",
    "    \"\"\"Save the trained model to disk.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'product_similarity_model_t{threshold}_{timestamp}.pkl'\n",
    "\n",
    "    try:\n",
    "        model_data = {\n",
    "            'vectorizer': model.vectorizer,\n",
    "            'product_names': model.product_names,\n",
    "            'product_groups': model.product_groups,\n",
    "            'group_representatives': model.group_representatives,\n",
    "            'similarity_threshold': model.similarity_threshold,\n",
    "            'training_date': datetime.now(),\n",
    "            'total_products': len(model.product_names)\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        print(f\"Model saved successfully as: {filename}\")\n",
    "        return filename\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a20f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_filename = save_model(\u001b[43mbest_model\u001b[49m, best_threshold)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_filename = save_model(best_model, best_threshold)\n",
    "print(f\"MODEL TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notes\n",
    "\n",
    "\n",
    "1. Load the model in your analysis.py:\n",
    "   ```python\n",
    "   from product_similarity_model import ProductSimilarityModel\n",
    "   model = ProductSimilarityModel.load_model('{model_filename}')\n",
    "   ```\n",
    "\n",
    "2. Replace your product data fetching:\n",
    "   ```python\n",
    "   # Instead of:\n",
    "   product_data = fetch_product_data(start_date, end_date)\n",
    "   \n",
    "   # Use:\n",
    "   consolidated_data = model.get_consolidated_products(\n",
    "       your_sales_data, \n",
    "       product_col='Product Name',\n",
    "       sales_col='Sales',\n",
    "       quantity_col='Quantity Sold'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. Your dashboard will now show consolidated product families instead of individual SKUs!\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nüéØ MODEL PERFORMANCE SUMMARY:\")\n",
    "if 'consolidated_sales' in locals():\n",
    "    print(f\"   ‚Ä¢ Reduced {len(sample_sales)} products to {len(consolidated_sales)} ({((len(sample_sales) - len(consolidated_sales)) / len(sample_sales) * 100):.1f}% reduction)\")\n",
    "    print(f\"   ‚Ä¢ Sales data fully preserved (100% accuracy)\")\n",
    "print(f\"   ‚Ä¢ Similarity threshold: {best_threshold}\")\n",
    "print(f\"   ‚Ä¢ Total groups created: {len(analysis_df)}\")\n",
    "print(f\"   ‚Ä¢ Multi-product groups: {len(analysis_df[analysis_df['Group_Size'] > 1])}\")\n",
    "\n",
    "print(f\"\\n‚ú® Your product analysis will now focus on product families rather than individual package sizes!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
